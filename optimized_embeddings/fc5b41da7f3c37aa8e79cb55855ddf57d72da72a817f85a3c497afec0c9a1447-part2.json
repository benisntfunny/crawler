{
    "id": "fc5b41da7f3c37aa8e79cb55855ddf57d72da72a817f85a3c497afec0c9a1447-part2",
    "url": "https://help.salesforce.com/s/articleView?id=000272311&language=en_US&r=https:%2F%2Fwww.google.com%2F&type=1",
    "content": "nullSalesforce Org Migrations overview\nPublish Date: Dec 19, 2023\nDescription\n\nUse this article to understand what Salesforce organization migrations are, and how they work within Salesforce's infrastructure.\n\n \n\nResolution\n\nWhat is an Org?\nAn org is the virtual space provided to an individual customer of Salesforce, and includes all customer data and applications. It’s composed of Systems of Record (SOR) that store customer data and metadata such as:\nRelational Database (DB)\nNoSQL Database (HBase)\nFileForce (Keystone)\nIn addition to these datastores there are other services that may store state for the org (search indexes in Solr, Domain Name System (DNS) for MyDomains, etc). This is not an exhaustive list of datastores and the list grows as we improve our infrastructure. For more information please refer to Salesforce architecture documentation.\nWhat is an Org Migration?\nAn org migration is a set of processes and technologies that move a production org from a source Salesforce instance to a target Salesforce instance. The org move is orchestrated by copying and/or regenerating customer data and metadata.\nWhat Happens During an Org Migration Window?\nDuring an org migration window, the org experiences a period of time where org access is in read-only mode . This is referred to as the org migration event window. This event window is required to let the migration processes make a consistent copy of the org’s data. The org migration tool orchestrates the copy of data from all source pod SORs to corresponding target pod SORs. Once all SOR data has been successfully copied, the org is ready to be activated on the target instance.\n\nThis document explains the mechanisms used to copy customer data from the 3 SORs.\nRelational Database (DB)\nSince Salesforce is a multi-tenant system, i.e. multiple tenants share database resources, standard database migration tools are not usable for our use case. Instead we built a custom data copy tool. This tool is run on Salesforce application servers. It has Copy and Validation stages.\nCopy\nData is stored in database tables. Table copy takes place in chunks. Chunks may be copied in parallel for performance. The following steps are involved in copying a chunk:\n\n1. Read the data from the source pod’s database using standard Structured Query Language (SQL) through Java Database Connectivity (JDBC)\n\n2. Transport the data to the target pod over HTTPS using end-to-end Transport Layer Security (TLS) encryption\n\n3. Insert the data into the target pod's database using standard SQL through JDBC. In the event of a chunk copy failure (failures could happen anywhere in the pipeline - source, network, target), the entire chunk copy is retried. There is no limit on the number of retries.\n\nValidation\nAfter all table data has been copied, we run a validation process. This process performs the following steps on every table:\n\n1. Run SQL queries using JDBC to collect the row count and checksum (of a subset of fields) on the source pod’s database and target pod’s database\n\n2. Compare row counts and checksums from source and target pods.\n\nIf all the validations complete successfully, the Relational Database copy has completed successfully.\nFileForce (Keystone)\nKeystone is composed of a metadata catalog and a store for extents. The metadata catalog is stored in the relational database as tables. The extent store is API driven and globally accessible -- target instances can access extents on source instances. \n\nDuring the org migration window, we copy just the metadata catalog from source pod to target pod using the mechanism described in the previous section, \"Relational Database (DB)\". After the migration the metadata catalog on the target instance will point to extents on the source instance, and requests to access files will fetch extents from the source instance initially. \n\nMigration of data on the extent stores from source to target instance is done asynchronously outside of the org migration window. This is done to reduce org read-only downtime.\nNoSQL Database (HBase)\nMigrating data between two Bigdata (HBase) clusters is a two pronged process. First, a replication process is enabled between source and target, so that any data written on the source side starts appearing at the target cluster. Second, the migration copies the data that already exists from source to target. This is a longer process and the duration depends on how much data exists to be copied over.\n\nSpecifically, when a org is scheduled for migration the steps done are:\n\n1. Setup cross cluster trust, so that processes on the source cluster can communicate with those on the target cluster. Our clusters are Kerberos secured and need cross trust setup.\n\n2. Initiate the replication start job such that data starts to flow to target cluster (only for the org).\n\n3. Initiate copy job to copy the existing data for the org.\n\n4. Copy metadata for the given org to the target cluster. This is needed for copying over Phoenix table views that won’t get replicated (in the current release).\n\n5. Ensure data copy jobs complete.\n\n6. Initiate teardown of trust between the two clusters.\n\nWhat Happens After an Org Migration?\nOn activation, the org will once again accept read-write requests serviced on the target pod. New writes will land on the target pod SOR stores. \n\nMigration of FileForce data on the source instance occurs asynchronously after the org has been activated. A periodic process on the target instance scans all Keystone metadata for the local store copies for extents. Since the recently migrated org will have extents still being accessed from source stores, a copy process enqueues store-to-store copy operations for all extent data for the migrated org. These operations get retried until success. For encrypted blobs, the store-to-store copy operations have an additional level of security, where decryption keys are stored as metadata rows and copied separately as part of the Relational Database copy process. \n\nGiven the asynchronous nature of the FileForce copy process copying to the target FileForce stores can take up to 2 weeks after the org is activated. At all times during this copy process uses have uninterrupted access to all their FileForce data. In certain org migrations performed for data residency purposes, once the Relational Database copy has completed, the FileForce copy process is immediately triggered to expedite the FileForce copy process. Even in this case it could still take up to 2 weeks for all data to be copied to the target instance. \nCleaning Up the Old Copy of the Org\nOnce all the org migration processes have run to completion, the copy of the org on the source instance is ready for deletion. Approximately 30 days after the migration, the “Migrated Org Delete” batch job will mark the copy of the org on the source instance as deleted. This will trigger the “Hard Delete” process to purge the data. For more information regarding the Hard Delete process please refer to the Deleting Data documentation.\nSandboxes\nSandboxes are not moved as part of an org migration. Sandboxes remain on the CS instance where they were located until they are refreshed. Existing sandboxes remain where they are until they are either deleted by the customer or the sandbox is refreshed, at which point the new sandbox org will be created in the same region as the production org. Metadata migration tools such as Change Sets and Ant continue to operate exactly as they did when the Sandbox and Production instances were in the same region as each other.\n \n\nSearch Operations\n\nDuring an org migration, search data is transferred from source servers to target servers. Source servers are backed up as part of standard search backup procedures. However, backups for an org scheduled for migration are executed with a higher priority to ensure that all data is up to date. No customer action is required.\n\nDuring the migration, search data backups are restored on the target servers. No data is removed from the source. Doing so allows Salesforce to easily roll back or abort operations, with no customer impact.\n\nThe restore process generally ends during off-business hours. For large orgs, the restore phase can take longer, which can impact search-dependent operations (for example - record lookup, content search).\n \nSee Also\nHow to Prepare for an Org Migration\n \nKnowledge Article Number\n\n000384180\n\nDID THIS ARTICLE SOLVE YOUR ISSUE?\nLet us know so we can improve!\nYes\nNo\n ",
    "title": "Understanding Salesforce Organization Migrations",
    "keywords": [
        "Salesforce migration",
        "Org migration",
        "Salesforce org move",
        "salesforce data migration",
        "data residency",
        "org migration process",
        "salesforce migration tools"
    ],
    "links": [
        "https://help.salesforce.com/sfsites/c/resource/HTCommunityCustomJS/HC_TbidAuth.js",
        "https://help.salesforce.com/s/support\",\"target\":\"_self\",\"label\":\"Contact",
        "https://help.salesforce.com/s/articleView?id=000334690&type=1&mode=1&language=en_US",
        "https://help.salesforce.com/s/articleView?id=How-to-Prepare-for-an-Org-Migration&language=en_US&type=1"
    ],
    "embedding": [
        0.10283594578504562,
        -0.12287680804729462,
        -0.6307331323623657,
        0.6673392057418823,
        -0.3071923851966858,
        -0.5473732948303223,
        -0.42685699462890625,
        -0.43693533539772034,
        -0.0032868534326553345,
        0.17680096626281738,
        0.10847305506467819,
        0.368472695350647,
        -0.9137762784957886,
        -0.5848342776298523,
        0.08733752369880676,
        -0.052587032318115234,
        1.8444459438323975,
        0.06497959047555923,
        -0.6107096672058105,
        0.06263557821512222,
        -0.13715995848178864,
        0.03374367207288742,
        -0.18780706822872162,
        -0.1883174180984497,
        -0.7176024317741394,
        0.4308309853076935,
        -0.1627880036830902,
        0.16653761267662048,
        0.49396181106567383,
        0.02949129045009613,
        -0.2396998554468155,
        -0.4700857996940613,
        -0.19027623534202576,
        0.13146468997001648,
        0.45814403891563416,
        0.1413535326719284,
        -0.44858473539352417,
        -0.12919820845127106,
        0.767144501209259,
        0.03730223327875137,
        -0.3111738860607147,
        0.41597291827201843,
        0.5910100340843201,
        0.7957092523574829,
        -0.28678080439567566,
        0.5260177254676819,
        -0.1499042809009552,
        -0.7220537066459656,
        0.307827889919281,
        -0.4286932647228241,
        -0.511650025844574,
        -0.1944005936384201,
        0.5526161193847656,
        0.4171048402786255,
        0.400842547416687,
        0.153349831700325,
        0.9355397820472717,
        -0.1011929139494896,
        0.23634138703346252,
        -0.3066965341567993,
        0.5370889902114868,
        -0.7301735281944275,
        -0.15987315773963928,
        0.08866441249847412,
        -0.5678917169570923,
        -0.6727707982063293,
        -0.34959131479263306,
        -0.642607569694519,
        -0.40886080265045166,
        0.12590961158275604,
        0.11565491557121277,
        0.24274159967899323,
        -0.3945043087005615,
        -0.4733855128288269,
        0.2798807621002197,
        -0.21081531047821045,
        -1.0178916454315186,
        0.9907224178314209,
        0.1138099879026413,
        -0.39792922139167786,
        -0.26037049293518066,
        0.964287519454956,
        -0.15583062171936035,
        0.5307802557945251,
        0.36728790402412415,
        -1.0057311058044434,
        -0.049858663231134415,
        0.766205906867981
    ]
}